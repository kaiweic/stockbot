# -*- coding: utf-8 -*-
"""Model Automate.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T7KoJh312SysO9up41gNwMXrF0dLM9hX
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.autograd import Variable
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from math import sqrt
import math, time
from sklearn.metrics import mean_squared_error
import glob


num_epochs=400
pred_epochs=2
look_back=30
ticker = "AAPL"
RL = True


for filepath in glob.iglob('./*'):
  if (filepath == "./sample_data"):
    continue
  ticker = filepath[2:-4]
  print(f"Processing {ticker}")
  RL = False
  doAll()
  RL = True
  doAll()
  break

def doAll():
  def getMergedData(ticker):
    dates = []
    all_sentiments = []
    with open(f"{ticker}_out.tsv") as sentiments:
      for sentiment in sentiments:
        date, day_sentiment = sentiment.split()
        dates.append(date)
        all_sentiments.append(float(day_sentiment))
    # print(len(dates) + len(all_sentiments), len(dates), len(all_sentiments))
    df = pd.DataFrame(np.array([dates, all_sentiments]).T, columns=['date', 'sentiment'])
    df.sentiment = df.sentiment.astype(np.float64)
    idx = pd.date_range('2010-01-01', '2020-05-23')
    df.index = pd.DatetimeIndex(df.date)
    df = df.reindex(idx, fill_value=0.5)


    stock = pd.read_csv(f"{ticker}.csv", index_col=0)
    stock.index = pd.DatetimeIndex(stock.date)
    stock = stock.merge(df, how='left', left_index=True, right_index=True)

    stock = stock[['open', 'high', 'low', 'close', 'volume', 'sentiment']]
    stock = stock.reset_index()
    stock.date = stock.date.astype(str)
    # print(stock.info())
    return stock


  scaler_o = MinMaxScaler(feature_range=(-1, 1))
  scaler_c = MinMaxScaler(feature_range=(-1, 1))
  scaler_h = MinMaxScaler(feature_range=(-1, 1))
  scaler_l = MinMaxScaler(feature_range=(-1, 1))
  scaler_v = MinMaxScaler(feature_range=(-1, 1))
  scaler_y = MinMaxScaler(feature_range=(-1, 1))
  def process_data(stock, look_back=30):
      stock = pd.read_csv(f"{stock}.csv", index_col=0)
      # stock = getMergedData(stock)
      # print(len(stock))
      stock['open'] = scaler_o.fit_transform(stock['open'].values.reshape(-1,1))
      stock['close'] = scaler_c.fit_transform(stock['close'].values.reshape(-1,1))
      stock['high'] = scaler_h.fit_transform(stock['high'].values.reshape(-1,1))
      stock['volume'] = scaler_v.fit_transform(stock['volume'].values.reshape(-1,1))
      stock['low'] = scaler_l.fit_transform(stock['low'].values.reshape(-1,1))
      
      y_dates = []
      dates = stock.date
      stock.drop(columns=['date'], inplace=True)

      dataset = []
      y = []
      for i in range(len(stock) - look_back):
          dataset.append([stock[i:i + look_back].to_numpy()])
          y.append(stock.loc[i + look_back]['close'])
          y_dates.append(dates[i + look_back])

      split_y = int(0.8 * len(stock))
      # split_y = None
      # for i, date in enumerate(y_dates):
      #   if date == '2016-01-04':
      #     split_y = i
      #     print("Triggered", len(dataset[split_y:]))
      #     assert len(dataset[split_y:]) == 1005, f"{len(dataset[split_y:])} {split_y}"

      assert len(y) == len(dataset)
      dataset = np.vstack(dataset)

      train = dataset[:split_y,:,:]
      test = dataset[split_y:,:,:]
      y = np.array(y).reshape((-1,1))

      train_x = torch.from_numpy(train[:,:,:5]).type(torch.Tensor)
      train_y = torch.from_numpy(y[:split_y]).type(torch.Tensor)
      test_x = torch.from_numpy(test[:,:,:5]).type(torch.Tensor)
      test_y = torch.from_numpy(y[split_y:]).type(torch.Tensor)
      
      return train_x, train_y, test_x, test_y

  train_x, train_y, test_x, test_y = process_data(ticker)
  # print(len(train_x), len(train_y), len(test_x), len(test_y))


  # Build model
  #####################
  input_dim = 5
  hidden_dim = 32
  num_layers = 2 
  output_dim = 1


  # Here we define our model as a class
  class LSTM(nn.Module):
      def __init__(self, input_dim, hidden_dim, num_layers, output_dim):
          super(LSTM, self).__init__()
          # Hidden dimensions
          self.hidden_dim = hidden_dim

          # Number of hidden layers
          self.num_layers = num_layers

          # Building your LSTM
          # batch_first=True causes input/output tensors to be of shape
          # (batch_dim, seq_dim, feature_dim)
          self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)

          # Readout layer
          self.fc = nn.Linear(hidden_dim, output_dim)

      def forward(self, x):
          # Initialize hidden state with zeros
          h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()

          # Initialize cell state
          c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()

          # One time step
          # We need to detach as we are doing truncated backpropagation through time (BPTT)
          # If we don't, we'll backprop all the way to the start even after going through another batch
          out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))

          # Index hidden state of last time step
          # out.size() --> 100, 28, 100
          # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! 
          out = self.fc(out[:, -1, :]) 
          # out.size() --> 100, 10
          return out
      
  model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)

  loss_fn = torch.nn.MSELoss(reduction="mean")

  optimiser = torch.optim.Adam(model.parameters(), lr=0.01)
  # print(model)
  # print(len(list(model.parameters())))
  # for i in range(len(list(model.parameters()))):
  #     print(list(model.parameters())[i].size())

  # Train model
  #####################
  remaining_days = len(test_y)

  hist = np.zeros(num_epochs + (pred_epochs * remaining_days if RL else 0))

  # Number of steps to unroll
  seq_dim =look_back-1  


  for t in range(num_epochs):
      # Initialise hidden state
      # Don't do this if you want your LSTM to be stateful
      #model.hidden = model.init_hidden()
      
      # Forward pass
      train_pred = model(train_x)
      loss = loss_fn(train_pred, train_y)
      # if t % 10 == 0 and t !=0:
      #     print("Epoch ", t, "MSE: ", loss.item())
      hist[t] = loss.item()

      # Zero out gradient, else they will accumulate between epochs
      optimiser.zero_grad()

      # Backward pass
      loss.backward()

      # Update parameters
      optimiser.step()


  if RL:
    pred_rl = []
    for day in range(remaining_days):
      pred_rl.append(model(test_x[0].reshape(1,look_back,-1)))
      train_x = torch.cat((train_x, test_x[0].view((1, look_back, -1))))
      test_x = test_x[1:]
      
      train_y = torch.cat((train_y, test_y[0].view(1,-1)))
      test_y = test_y[1:]
      for t in range(day * pred_epochs + num_epochs, (day + 1) * pred_epochs + num_epochs):
          # Initialise hidden state
          # Don't do this if you want your LSTM to be stateful
          #model.hidden = model.init_hidden()

          # Forward pass
          train_pred = model(train_x)
          loss = loss_fn(train_pred, train_y)
          # if t % 10 == 0 and t !=0:
          #     print("Epoch ", t, "MSE: ", loss.item())
          hist[t] = loss.item()

          # Zero out gradient, else they will accumulate between epochs
          optimiser.zero_grad()

          # Backward pass
          loss.backward()

          # Update parameters
          optimiser.step()


  # if not RL:
    # plt.plot(train_pred.detach().numpy(), label="Preds")
    # plt.plot(train_y.detach().numpy(), label="Data")
    # plt.legend()
    # plt.show()

  # plt.plot(hist, label="Training loss")
  # plt.legend()
  # plt.show()
  # plt.close()

  # make predictions
  if not RL:
    test_pred = model(test_x)

  if not RL:
    # invert predictions
    train_pred = train_pred.detach().numpy()
    train_y = train_y.detach().numpy()
    test_pred = test_pred.detach().numpy()
    test_y = test_y.detach().numpy()

    train_pred = scaler_c.inverse_transform(train_pred)
    train_y = scaler_c.inverse_transform(train_y)
    test_pred = scaler_c.inverse_transform(test_pred)
    test_y = scaler_c.inverse_transform(test_y)

    # calculate root mean squared error
    trainScore = math.sqrt(mean_squared_error(train_y[:,0], train_pred[:,0]))
    print('Train Score: %.2f RMSE' % (trainScore))
    testScore = math.sqrt(mean_squared_error(test_y[:,0], test_pred[:,0]))
    print('Test Score: %.2f RMSE' % (testScore))

    df = pd.read_csv(f"{ticker}.csv", index_col=0)
    df_dates = pd.to_datetime(df.date)
    df = df.close
    # print(df_dates)

    # shift train predictions for plotting
    trainPredictPlot = np.empty_like(df)
    trainPredictPlot[:] = np.nan
    # print(trainPredictPlot.shape, df.shape, train_pred.shape)
    trainPredictPlot[look_back:len(train_pred)+look_back] = train_pred[:,0]

    # shift test predictions for plotting
    testPredictPlot = np.empty_like(df)
    testPredictPlot[:] = np.nan
    # print(test_y.shape, len(df) - remaining_days)
    testPredictPlot[len(df) - remaining_days:] = test_pred[:, 0]

    # plot baseline and predictions
    plt.figure(figsize=(15,8))
    plt.plot(df_dates, df.to_numpy().reshape((-1,1)), label="Actual")
    plt.plot(df_dates, trainPredictPlot, label="Train") 
    plt.plot(df_dates[len(df) - len(testPredictPlot):], testPredictPlot, label="Test")
    
    sma = df.rolling(10).mean()
    sma = sma.shift(1)
    plt.plot(df_dates[len(df) - len(testPredictPlot):], sma[len(df) - len(testPredictPlot):], label="SMA")
    print(f"SMA RMSE: {np.sqrt(np.mean((df[len(df) - len(testPredictPlot):] - sma[len(df) - len(testPredictPlot):]) ** 2))}")

    
    plt.xlabel("Years")
    plt.ylabel("Price")
    plt.title(f"{ticker} Price vs Year Plots for Training and Testing (No Iterative Training)")
    plt.grid()
    plt.legend()
    plt.show()
  else:
    rl_df = pd.read_csv(f"{ticker}.csv", index_col=0)
    rl_date = pd.to_datetime(rl_df.date)
    rl_df = rl_df.close
    rl_pred = np.empty_like(rl_df)
    rl_pred[:] = np.nan
    result = []
    for r in pred_rl:
      result.append(r.detach().numpy())

    result = scaler_c.inverse_transform(np.vstack(result))
    rl_pred[len(rl_df) - remaining_days:] = result[:, 0]
    # print(len(rl_df))
    plt.figure(figsize=(15,8))

    plt.ylabel("Price")
    plt.xlabel("Years")
    plt.title(f"{ticker} Price vs Year Plots for Training and Testing (Iterative Training)")
    plt.plot(rl_date, rl_df.to_numpy(), label="Actual")
    plt.plot(rl_date[len(rl_df) - len(rl_pred):], rl_pred, label="Prediction")


    sma = rl_df.rolling(10).mean()
    sma = sma.shift(1)
    plt.plot(rl_date[len(rl_df) - len(result):], sma[len(rl_df) - len(result):], label="SMA")

    plt.grid()
    plt.legend()
    plt.setp(plt.gca().xaxis.get_majorticklabels())
    plt.show()

    tr_x, tr_y, ts_x, ts_y = process_data(ticker, look_back=30)
    ts_y = scaler_c.inverse_transform(ts_y.detach().numpy())
    testScore = math.sqrt(mean_squared_error(ts_y[:,0], result[:,0]))
    
    print('Test Score: %.2f RMSE' % (testScore))
    rl_price = pd.DataFrame(np.array([rl_date[len(rl_df) - len(rl_pred):], rl_df[len(rl_df) - len(rl_pred):].to_numpy(), rl_pred]).T, columns=["date", "close", "prediction"])
    rl_price = rl_price.dropna()
    rl_price.to_csv(f"{ticker}_RL.csv")

df.date[int(len(df.close)*0.8):]



df = pd.read_csv("AAPL.csv")

df["SMA"] = df.close.rolling(1).mean()

df

